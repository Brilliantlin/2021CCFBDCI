model_config:
  model_name: &model_name 'attention_lac_ratio0'
  init_ckpt: 'ernie-gram-zh' #
  init_from_ckpt: ~
  num_labels: 2
  rdrop_coef: 0
  dropout: 0.1

  lac_vocab_size: 49
  gru_emb_dim: 4
  gru_hidden_size: 8
  direction: 2 #1 or 2
  gru_layers: 1
  gru_dropout_rate: 0.1
  dep_vocab_size: 29


data_config:
  train_set: '../../data/train.txt'
  dev_set: '../../data/dev.txt'
  max_seq_length: 70
  ratio: 0

train_config:
  fold: 7
  train_batch_size: 128
  eval_batch_size: 128
  max_steps: -1
  learning_rate: 2.0e-5
  weight_decay: 1.0e-3
  epochs: 5
  eval_step: 300
  save_step: 10000
  warmup_proportion: 0.2
  init_from_ckpt: ''
  seed: 2022
  device: gpu
  silent: False
  max_grad_norm: ~
debug: False
logger_name: *model_name

